# Python的中文处理库：jieba
jieba就是这样一个非常好用的中文分词工具，但是其功能比分词要强大很多
## 基本分词
jieba.cut 以及 jieba.cut_for_search 返回的结构都是一个可迭代的 generator，可以使用 for 循环来获得分词后得到的每一个词语(unicode)  
  
**jieba.cut** 方法接受三个输入参数:
* 需要分词的字符串
* cut_all 参数用来控制是否采用全模式   (*Full Mode表示将所有可能的分词全部列出*)
* HMM 参数用来控制是否使用 HMM 模型  
  
**jieba.cut_for_search** 方法接受两个参数：
* 需要分词的字符串
* 是否使用 HMM 模型。  

**【该方法适合用于搜索引擎构建倒排索引的分词，粒度比较细】**  

  
  
    
**添加用户自定义字典**  

很多时候我们需要针对自己的场景进行分词，会有一些领域内的专有词汇。  


1.可以用jieba.load_userdict(file_name)加载用户字典  
2.少量的词汇可以自己用下面方法手动添加：  
* 用 add_word(word, freq=None, tag=None) 和 del_word(word) 在程序中动态修改词典
* 用 suggest_freq(segment, tune=True) 可调节单个词语的词频，使其能（或不能）被分出来。  

```python
print('/'.join(jieba.cut('如果放到旧字典中将出错。', HMM=False))) 
```

如果/放到/旧/字典/中将/出错/。
```python
jieba.suggest_freq(('中', '将'), True)
print('/'.join(jieba.cut('如果放到旧字典中将出错。', HMM=False)))
```
如果/放到/旧/字典/中/将/出错/。
  
  
## 关键词提取

#### 基于TF-IDF的关键词提取
基本思想：该词在当前文档的出现频率和全局文档的出现频率之比  

`import jieba.analyse`  
* `jieba.analyse.extract_tags(sentence, topK=20, withWeight=False, allowPOS=()) `
> * sentence 为待提取的文本
> * topK 为返回几个 TF/IDF 权重最大的关键词，默认值为 20
> * withWeight 为是否一并返回关键词权重值，默认值为 False
> * allowPOS 仅包括指定词性的词，默认值为空，即不筛选  
  
#### 基于TextRank的关键词提取
基本思想:（建议语料较大时使用）  

* 将待抽取关键词的文本进行分词  
* 以固定窗口大小(默认为5，通过span属性调整)，词之间的共现关系，构建图  
* 计算图中节点的PageRank，注意是无向带权图  

`jieba.analyse.textrank(sentence, topK=20, withWeight=False)`  

直接使用，接口相同，注意默认过滤词性。  

`jieba.analyse.TextRank()`  

新建自定义 TextRank 实例
  
  
## 词性标注  


* `jieba.posseg.POSTokenizer(tokenizer=None)` 新建自定义分词器，tokenizer 参数可指定内部使用的 jieba.Tokenizer 分词器。jieba.posseg.dt 为默认词性标注分词器。  
* 标注句子分词后每个词的词性，采用和 ictclas 兼容的标记法。  
* 具体的词性对照表参见[计算所汉语词性标记集](http://ictclas.nlpir.org/nlpir/html/readme.htm) 
```python
import jieba.posseg as pseg
words = pseg.cut("我爱自然语言处理")
for word, flag in words:
    print('%s %s' % (word, flag))

```
我 r  
爱 v  
自然语言 l  
处理 v  
  
  
  
  
## 并行分词  
  
  
原理：将目标文本按行分隔后，把各行文本分配到多个 Python 进程并行分词，然后归并结果，从而获得分词速度的可观提升 基于 python 自带的 multiprocessing 模块，目前暂不支持 Windows  
  
  

用法：  


> `jieba.enable_parallel(4)` # 开启并行分词模式，参数为并行进程数  
> `jieba.disable_parallel()` # 关闭并行分词模式  

  
  

**注意：并行分词仅支持默认分词器 jieba.dt 和 jieba.posseg.dt**
